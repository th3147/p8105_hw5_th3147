p8105_hw5_th3147
================
Te-Hsuan Huang
2025-11-14

# Loading the package

``` r
library(tidyverse)
```

    ## Warning: package 'tidyverse' was built under R version 4.5.2

``` r
library(broom)
```

# Problem 1

## Make function

``` r
simulate_birthday_match <- function(n) {

  birthdays <- sample(1:365, size = n, replace = TRUE)
  has_duplicate <- length(birthdays) != length(unique(birthdays))

  # 3. Return TRUE (match found) or FALSE (no match)
  return(has_duplicate)
}

# Define parameters
min_n <- 2
max_n <- 50
n_simulations <- 10000

# Create a sequence of group sizes to test
group_sizes <- min_n:max_n

# Initialize a vector to store the calculated probabilities
probabilities <- numeric(length(group_sizes))

for (i in 1:length(group_sizes)) {
  n <- group_sizes[i]

  # Run the simulation n_simulations times for the current group size
  results <- replicate(n_simulations, simulate_birthday_match(n))

  # Compute the probability:
  # The mean() of a logical vector (TRUE/FALSE) calculates the proportion of TRUEs
  probability_of_match <- mean(results)

  # Store the result
  probabilities[i] <- probability_of_match
}

# Create a data frame for plotting (good practice)
probability_data <- data.frame(
  group_size = group_sizes,
  probability = probabilities
)
```

## Draw a plot

``` r
# Make a plot
plot(probability_data$group_size, probability_data$probability,
     type = "b", # 'b' for both points and lines
     col = "blue",
     main = "Probability of Shared Birthday vs. Group Size (Simulated)",
     xlab = "Group Size (n)",
     ylab = "P(At least two people share a birthday)",
     ylim = c(0, 1) # Ensure the y-axis spans 0 to 1
)
abline(h = 0.5, col = "red", lty = 2) # Add a line at P=0.5
text(20, 0.55, "50% Probability Line", col = "red", pos = 4)
```

![](p8105_hw5_th3147_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

# Problem 2

## Make the function

``` r
analyze_datasets <- function(mu, sd=5, n_simulations = 5000, sample_size = 30) {
  tibble(
    simulation_id = 1:n_simulations,
    data = map(1:n_simulations, ~ rnorm(n = sample_size, mean = mu, sd = sd)),
    t_test_results = map(
      .x = data,
      .f = ~ tidy(t.test(.x, mu = 0)) # H0: mu = 0
    )
  )
}
```

\##Test the function

``` r
final_sim_results_nested0 <- analyze_datasets(mu = 0)
final_sim_results_nested1 <- analyze_datasets(mu = 1)
final_sim_results_nested2 <- analyze_datasets(mu = 2)
final_sim_results_nested3 <- analyze_datasets(mu = 3)
final_sim_results_nested4 <- analyze_datasets(mu = 4)
final_sim_results_nested5 <- analyze_datasets(mu = 5)
final_sim_results_nested6 <- analyze_datasets(mu = 6)
```

## Make the dataset together

``` r
all_results_list <- list(
  `0` = final_sim_results_nested0,
  `1` = final_sim_results_nested1,
  `2` = final_sim_results_nested2,
  `3` = final_sim_results_nested3,
  `4` = final_sim_results_nested4,
  `5` = final_sim_results_nested5,
  `6` = final_sim_results_nested6
)

power_curve_data <- 
  all_results_list |> 
  bind_rows(.id = "true_mu_value")  |> 
  mutate(true_mu_value = as.numeric(true_mu_value))  |> 
  unnest(t_test_results) |> 
  group_by(true_mu_value) |> 
  summarise(
    average_estimate = mean(estimate, na.rm = TRUE),
    conditional_average_estimate = mean(estimate[p.value < 0.05], na.rm = TRUE),
    power = mean(p.value < 0.05, na.rm = TRUE),
    .groups = "drop"
  )
```

- .groups = “drop”: remove the grouping structure after the summary
  calculation

## Draw the first plot

``` r
power_plot <- ggplot(power_curve_data, aes(x = true_mu_value, y = power)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
  labs(
    title = "Statistical Power Curve for One-Sample T-Test",
    subtitle = "Power (Rejection Rate of H0: μ = 0) vs. True Population Mean (μ)",
    x = expression("True Value of Mean"),
    y = "Statistical Power"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title.position = "plot",
    panel.grid.minor = element_blank()
  )

power_plot
```

![](p8105_hw5_th3147_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

- When the effect size increases, the power of the test also increases.
  However, once the effect size reaches 4, the power values all appear
  to be the same.

## Draw the second plot

``` r
estimate_plot <- 
  ggplot(power_curve_data, aes(x = true_mu_value, y = average_estimate)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  scale_x_continuous(name = expression("True Value of Mean")) +
  scale_y_continuous(name = expression("Average Estimated Mean")) +
  labs(
    title = "Average Estimated Mean vs. True Mean (Assessing Bias)",
    subtitle = "The sample mean is an unbiased estimator, so the line should follow y=x."
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title.position = "plot",
    panel.grid.minor = element_blank()
  )
estimate_plot
```

![](p8105_hw5_th3147_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->

## Draw the second plot but condition on group which the null was rejected

``` r
conditional_estimate_plot <- 
  ggplot(power_curve_data, aes(x = true_mu_value, y = conditional_average_estimate)) +
  # Add the simulation results
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  scale_x_continuous(name = expression("True Value of Mean")) +
  scale_y_continuous(name = "Average Estimated Mean") +
  labs(
    title = "Average Estimated Mean in Rejected Samples") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title.position = "plot",
    panel.grid.minor = element_blank()
  )

conditional_estimate_plot
```

![](p8105_hw5_th3147_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->

- No, it did not match. The plot shows that when a study is small mu,
  the estimates that manage to achieve significance are exaggerated.

- The bias occurs because we are only averaging the estimates from the
  simulations where a specific condition was met: the null hypothesis
  was rejected. In other words, When the true mu is small, the power is
  low. This means most of your 5000 simulated samples did not reject the
  null hypothesis. To be more specific, at mu=0, the only samples
  rejected are the rare, extreme outliers that resulted in a large
  sample mean by chance. Averaging only these outliers guarantees the
  estimate will be far above 0.

# Problem 3

## Loading the dataset

``` r
homicide_data <- read_csv("./homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

## Clean the data

``` r
homicide_data_filtered <- 
    homicide_data |> 
    janitor::clean_names() |> 
    mutate(
    state = str_replace(state, "wI", "WI")
  ) 
```

## Create a new variable

``` r
homicide_data_modified <-
  homicide_data_filtered  |> 
  mutate(
    city_state = str_c(city, state, sep = ", ")
  )
```

## Make a summary

``` r
homicide_summary <-
  homicide_data_modified |> 
  group_by(city_state) |> 
  summarise(
    homicides_solved = sum(disposition == "Closed by arrest"),
    homicides_unsolved = sum(disposition == "Closed without arrest" | disposition == "Open/No arrest"),
    total_homicides = n()
  ) |> 
  ungroup() |> 
  arrange(desc(total_homicides)) |> 
  filter(city_state != "Tulsa, AL")

head(homicide_summary,3)
```

    ## # A tibble: 3 × 4
    ##   city_state       homicides_solved homicides_unsolved total_homicides
    ##   <chr>                       <int>              <int>           <int>
    ## 1 Chicago, IL                  1462               4073            5535
    ## 2 Philadelphia, PA             1677               1360            3037
    ## 3 Houston, TX                  1449               1493            2942

``` r
homicide_summary_solved <-
  homicide_summary |> 
  arrange(desc(homicides_solved))

head(homicide_summary_solved,3)
```

    ## # A tibble: 3 × 4
    ##   city_state       homicides_solved homicides_unsolved total_homicides
    ##   <chr>                       <int>              <int>           <int>
    ## 1 Philadelphia, PA             1677               1360            3037
    ## 2 Chicago, IL                  1462               4073            5535
    ## 3 Houston, TX                  1449               1493            2942

``` r
homicide_summary_unsolved<-
  homicide_summary |> 
  arrange(desc(homicides_unsolved))

head(homicide_summary_unsolved,3)
```

    ## # A tibble: 3 × 4
    ##   city_state    homicides_solved homicides_unsolved total_homicides
    ##   <chr>                    <int>              <int>           <int>
    ## 1 Chicago, IL               1462               4073            5535
    ## 2 Baltimore, MD             1002               1825            2827
    ## 3 Houston, TX               1449               1493            2942

- The dataset showed that the top three number of homicides are Chicago,
  Philadelphia, and Houston.

- The dataset showed that the top three number of solved homicides are
  Philadelphia, Chicago, and Houston.

- The dataset showed that the top three number of unsolved homicides are
  Chicago, Baltimore, and Houston.

## Make a function

``` r
get_prop_estimates <- function(x, n) {
  prop_test_result <- prop.test(x = x, n = n)
  prop_test_result  |> 
    tidy()  |> 
    select(
      prop_estimate = estimate,
      conf_low = conf.low,
      conf_high = conf.high
    )
}
```

## Make a table

``` r
city_prop_estimates_tidy <- 
  homicide_summary |> 
  
  # A. Create a list column of tidy tibbles
  # map2 applies your function row-wise
  # The result is a list column where each element is a 1-row tibble.
  mutate(
    tidy_results = purrr::map2(
      .x = homicides_unsolved,
      .y = total_homicides,
      .f = ~get_prop_estimates(x = .x, n = .y)
    )
  ) |> 
  # This expands the list of 1-row tibbles into the main dataframe.
  unnest(tidy_results)  |> 
  select(
    city_state,
    total_homicides,
    homicides_unsolved,
    prop_estimate,
    conf_low,
    conf_high
  ) |> 
  arrange(desc(total_homicides))

city_prop_estimates_tidy
```

    ## # A tibble: 50 × 6
    ##    city_state       total_homicides homicides_unsolved prop_estimate conf_low
    ##    <chr>                      <int>              <int>         <dbl>    <dbl>
    ##  1 Chicago, IL                 5535               4073         0.736    0.724
    ##  2 Philadelphia, PA            3037               1360         0.448    0.430
    ##  3 Houston, TX                 2942               1493         0.507    0.489
    ##  4 Baltimore, MD               2827               1825         0.646    0.628
    ##  5 Detroit, MI                 2519               1482         0.588    0.569
    ##  6 Los Angeles, CA             2257               1106         0.490    0.469
    ##  7 St. Louis, MO               1677                905         0.540    0.515
    ##  8 Dallas, TX                  1567                754         0.481    0.456
    ##  9 Memphis, TN                 1514                483         0.319    0.296
    ## 10 New Orleans, LA             1434                930         0.649    0.623
    ## # ℹ 40 more rows
    ## # ℹ 1 more variable: conf_high <dbl>

## Make a plot

### Arrange the dataset by porpotional value

``` r
plot_data <- 
  city_prop_estimates_tidy |> 
  mutate(
    city_state_ordered = fct_reorder(city_state, prop_estimate)
  )
```

### Get the plot

``` r
proportion_plot <- 
  plot_data |> 
  ggplot(aes(x = prop_estimate, y = city_state_ordered)) +
  
  # 1. Add the confidence interval error bars
  geom_errorbar(
    aes(xmin = conf_low, xmax = conf_high),
    width = 0.2,
    color = "gray60"
  ) +
  geom_point() +
  scale_x_continuous(
    labels = scales::percent, 
    limits = c(0, 1)          
  ) +
  labs(
    title = "Estimated Proportion of Unsolved Homicides by City",
    subtitle = "Points show proportion; bars show 95% Confidence Intervals",
    x = "Proportion Unsolved",
    y = "City, State"
  ) +
  
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 5),
    plot.title.position = "plot"
  )

proportion_plot
```

![](p8105_hw5_th3147_files/figure-gfm/unnamed-chunk-17-1.png)<!-- -->
